name: CI

on:
  push:
    branches: ["**"]

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v3
      - name: Install dependencies
        run: |
          if [[ "$RUNNER_OS" == "Linux" ]]; then
            sudo apt-get update
            sudo apt-get install -y cmake build-essential clang-format \
              libfmt-dev libgtest-dev lcov libboost-dev
          elif [[ "$RUNNER_OS" == "macOS" ]]; then
            brew update
            brew install cmake clang-format fmt googletest boost
          fi
      - name: Configure
        run: |
          if [[ "$RUNNER_OS" == "Linux" ]]; then
            cmake -S . -B build -DENABLE_COVERAGE=ON -DWI_BUILD_TESTS=ON -DWI_BUILD_BENCHMARKS=OFF
          else
            cmake -S . -B build -DWI_BUILD_TESTS=ON -DWI_BUILD_BENCHMARKS=OFF
          fi
      - name: Build
        run: cmake --build build --config Debug
      - name: Run tests
        run: |
          cd build
          ctest --output-on-failure
      - name: Generate coverage
        if: matrix.os == 'ubuntu-latest'
        run: |
          lcov --capture --directory build --output-file coverage.info --ignore-errors mismatch
          lcov --remove coverage.info '/usr/*' '*/tests/*' --output-file coverage.info
          lcov --list coverage.info
      - name: Check code format
        run: |
          FILES=$(git ls-files '*.cpp' '*.hpp' '*.h')
          if [ -n "$FILES" ]; then
            clang-format -i --style=file $FILES
            git diff --exit-code
          fi

  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake build-essential libfmt-dev libbenchmark-dev libboost-dev
      - name: Configure
        run: cmake -S . -B build -DWI_BUILD_TESTS=OFF -DWI_BUILD_BENCHMARKS=ON
      - name: Build
        run: cmake --build build --config Release
      - name: Run benchmarks
        run: |
          build/perf_cxx17 --benchmark_min_time=0.01s
          build/perf_cxx11 --benchmark_min_time=0.01s
          build/perf_compare_int256_cxx11 --benchmark_min_time=0.01s --benchmark_out=perf_compare_int256_cxx11.json --benchmark_out_format=json
      - name: Summarize perf_compare_int256_cxx11
        run: |
          python - <<'PYTHON'
          import json
          import os
          with open('perf_compare_int256_cxx11.json') as f:
              data = json.load(f)
          lines = ["| Benchmark | CPU Time (ns) | Real Time (ns) |", "|---|---:|---:|"]
          for b in data.get("benchmarks", []):
              name = b.get("name", "")
              cpu = b.get("cpu_time", 0.0)
              real = b.get("real_time", 0.0)
              lines.append(f"| {name} | {cpu:.2f} | {real:.2f} |")
          with open(os.environ["GITHUB_STEP_SUMMARY"], "a") as out:
              out.write("### perf_compare_int256_cxx11\n")
              out.write("\n".join(lines) + "\n")
          PYTHON
